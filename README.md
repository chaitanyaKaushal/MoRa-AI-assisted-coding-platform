# MoRa: An AI-Assisted Coding Platform - Powered by Finetuned LLM.

## Statement
This project studies whether a large language model can be used to reliably transform vague, human-written programming problems into executable, self-consistent problem artifacts — formal specifications, test suites, and reference solutions — and how correctness can be operationally evaluated without access to a ground-truth oracle at inference time.

## Use-case: AI-assited Coding Platform
This project builds an AI system that takes a vague, human-written recollection of a coding problem and turns it into a fully usable coding environment: a formal problem specification, a test suite, and a reference solution. The system does not rely on externally verified ground truth; instead, it enforces correctness through internal consistency. Test cases are generated by an LLM and filtered by an oracle, and correctness is judged by comparing an LLM-generated brute-force solution against an LLM-generated optimized solution, treating brute force as the operational truth. When inconsistencies arise, the system attempts automated self-healing, and when that fails, it allows human truth injection via counterexamples. **The result is not a formally verified system, but a pragmatic, execution-based correctness pipeline that works well in practice, fails transparently when it doesn’t, and is explicitly designed for real interview recall scenarios rather than theoretical guarantees.**

**It is extremely useful for DSA round preparations where students often refer past-interview experiences, only to find there is no associated Leetcode/CodeForces problem available for it. Our LLM can deal with the different problem flavors and provide you a coding playground.**

## [Design Document](./Design%20Document.pdf)
This project presents an **AI-assisted coding practice platform** that reconstructs a complete problem-solving environment—formal specification, test suite, reference solutions, and interactive practice—from minimal or vague user input, such as how candidates recall interview problems days later. By modeling four realistic user personas (layman, conversational, technical shorthand, and implementation-specific), the system robustly translates ambiguous natural-language descriptions into precise formal specifications using a fine-tuned LLM trained on real LeetCode problems. A multi-stage pipeline then generates and validates test cases, produces brute-force and optimized reference solutions, and iteratively self-heals inconsistencies through execution-based checks and user-provided counterexamples. Rather than assuming access to ground-truth answers, the platform defines correctness operationally via consistency between independently generated solutions and tests, emphasizing reliability, transparency, and practical usability for interview preparation and coding practice.

## [Workflow Document](./Workflow%20Document.pdf)
The inference pipeline converts a vague user query into a complete, executable coding task through a staged, execution-driven process rather than formal verification. First, a fine-tuned LLM translates free-form input into a strict JSON formal specification (Stage 3). Next, another LLM acts as a test oracle, generating categorized test inputs and a constraint-checking validator; invalid or insufficient tests trigger retries until a minimum viable test suite is produced (Stage 4). The system then generates both a brute-force and an optimized solution using LLMs and treats the brute-force output as operational ground truth, resolving mismatches via a bounded self-healing loop that edits the optimized solution until consensus is reached or retries are exhausted (Stage 5). User-submitted solutions are evaluated against the finalized tests in a sandboxed environment (Stage 6). When users provide counterexamples, the system validates them against constraints, injects them as hard truth, regenerates tests and solutions, and updates stored artifacts—making correctness an evolving, execution-consistent property rather than a formally proven one.

## [Architecture Components](./Architecture%20Document.pdf)
The system is a FastAPI-based client–server platform that operationalizes a multi-stage LLM pipeline rather than a formally verified compiler or solver. A web client interacts with a backend API that orchestrates problem generation, test synthesis, solution generation, and evaluation. All semantic reasoning is delegated to a fine-tuned GPT-4.1 model, while the backend acts as a deterministic controller: it enforces schemas, executes generated Python code in sandboxes, applies timeouts, retries failed stages, and persists artifacts in a Postgres database. Core logic is implemented in an orchestrator layer that sequences LLM calls, validates outputs, and manages circular self-healing when tests or solutions conflict. The database serves as a cache and audit log for problems, tests, reference solutions, and user submissions rather than a source of truth for correctness. Correctness emerges empirically from execution agreement (brute-force vs optimized vs user-provided truth), not from static proofs. Deployment is a single Uvicorn service exposing streaming APIs, with scalability and reliability achieved through retries, timeouts, and explicit failure handling rather than distributed guarantees.

## [Implementation Document](./Implementation%20Document.pdf)
The system is implemented as a multi-stage, safety-first pipeline where all LLM outputs are treated as untrusted and rigorously validated. Each stage uses carefully controlled prompts and temperatures, with deterministic specification generation, constrained test synthesis, and low-variance solution generation. All generated code (tests, reference solutions, and user submissions) is statically checked via AST analysis, executed in isolated subprocesses, and guarded by strict timeouts to prevent hangs or unsafe behavior. Correctness is enforced through differential testing between an LLM-generated brute-force solution (treated as practical ground truth) and an optimized solution, with bounded self-healing loops triggered on mismatches. Circuit breakers, retries, and fallback mechanisms ensure termination, while user-provided counterexamples can be injected as truth to repair errors—prioritizing robustness and controlled failure over claims of formal verification.

## [Healing and Retries](./Healing%20and%20Retries.pdf)
The system enforces correctness through strictly bounded retries and fail-closed healing mechanisms that treat all LLM outputs as untrusted proposals. Test generation, reference solutions, and optimizations are repeatedly validated via static checks, sandboxed execution, differential testing against a brute-force baseline, and hard timeouts. When mismatches occur, the platform performs localized program repair with strict retry limits, aborting once bounds are reached rather than guessing correctness. User-provided counterexamples can inject ground truth, but even these are constrained by the same termination guarantees. At every stage, the platform—not the LLM—decides acceptance, ensuring robustness, reproducibility, and controlled failure over illusory correctness.

## [Evaluation and Benchmarks](./Evaluation.pdf)